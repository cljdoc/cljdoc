= cljdoc Operations
:toclevels: 5
:toc:

== Audience
You are on the cljdoc ops team or want to learn about cljdoc infracstructure.

This config is specific to cljdoc production.
If you want to bring up your own cljdoc-like service, you can use our config as a basis, but you'll need to make changes.

== Prerequsites

[#secrets]
=== Secrets

==== Exoscale Creds

Terraform access to Exoscale is configured in `ops/exoscale/infrastructure/secrets.tfvars`:

[source,hcl]
----
exoscale_api_key = <your key here>
exoscale_api_secret = <your value here>
----
IMPORTANT: Protect this file, it contains secrets.
Good idea to `chmod 600 secrets.tfvars`.

Packer can read this file format but is picky about the filename, so we'll create a soft link to satisfy it:

[source,sh]
----
cd ops/exoscale/image
ln -s ../infrastructure/secrets.tfvars ./secrets.pkrvars.hcl
----

==== Authorized SSH Keys

Authorized keys are stored in a non-version controlled tfvars file.

The structure is:

[shource,sh]
----
base_authorized_key = "<base pub key here>"

additional_authorized_keys = [
  "<additional pub key 1 here>",
  "<additional pub key 2 here>"
]
----

Terraform might suggest that changes can be applied in place, but you'll need to `taint` the compute instance, and `apply` it for changes to be applied correctly.
Yup, this means we can't currently change authorized ssh keys without recreating the cljdoc compute instance.

This file will be shared securely, as needed, with cljdoc ops team members.

=== Software

You'll need the following software installed:

* https://www.packer.io[Packer] to create our server host image
* https://www.terraform.io[Terraform] to create and manage our infrastructure

Optionally install:

* https://www.nomadproject.io/[Nomad] if you want to run it locally against production, see <<accessing-nomad>>.

== Overview
Here's what cljdoc looks like from an ops perspective.

image::ops-overview.drawio.svg[]

== Philosophy for Changes

To stay sane, we want to avoid making any infrastructure changes directly to prod.
All changes should be carried out with Packer, Terraform, and automatic deployments carried out by our CircleCI job.

== Cljdoc Host Image (Packer)
The cljdoc host image is our single node image that will host all of our compute infrastructure.

We use Packer to create machine images for the cljdoc host compute image.
Sources are under link:./exoscale/image/[]
The images are based on an Exoscale Debian template.
The following software is installed:

* https://www.docker.com/[Docker] - hosts our traefik load balancer and cljdoc server
* https://github.com/hashicorp/nomad[Nomad] - orchestrates deployment of cljdoc and traefik
* https://www.consul.io/[Consul] - used by traefik and nomad for config and discovery

Of interest:

* link:./exoscale/image/debian-cljdoc.pkr.hcl[] - tells Packer how to build our image
* link:./exoscale/image/conf[] - some service config files installed by our Packer config

=== Creating a Cljdoc Host Image (Packer)

TIP: Packer refers to secrets you setup in <<secrets>>.

Change to the appropriate dir:
[source,sh]
----
cd ops/exoscale/image
----

Optionally validate:
[source,sh]
----
packer validate -var-file=secrets.pkrvars.hcl debian-cljdoc.pkr.hcl
----

And finally build on Exoscale
[source,sh]
----
packer build -var-file=secrets.pkrvars.hcl debian-cljdoc.pkr.hcl
----

This will create a new `debian-cljdoc` image template on Exoscale.
Exoscale will automatically pick the latest `debian-cljdoc` template.

TIP: It's a good idea to occassionally log into the https://portal.exoscale.com[Exoscale Portal] and under Compute->Templates, delete old unused `debian-cljdoc` templates.

== Infrastructure (Terraform)

We use Terraform to create resources on Exoscale, including:

* Simple Object Store bucket for cljdoc backups
* Compute instance for the cljdoc server host
* DNS config

NOTE: The `cljdoc.org` domain should be configured to point to Exoscale.

=== Common Commands

TIP: These commands require secrets to be configured as described in <<secrets>>.

First, change to the appropriate dir:
[source,sh]
----
cd ops/exoscale/infrastructure
----

To view the plan terraform would carry out:
[source,sh]
----
terraform plan -var-file=secrets.tfvars
----

To carry out the plan:
[source,sh]
----
terraform apply -var-file=secrets.tfvars
----

To sync the server state back to terraform:
[source,sh]
----
terraform refresh -var-file=secrets.tfvars
----

Retrieving outputs:

[source,sh]
----
terraform output
terraform output -json
terraform output cljdoc_static_ip
----

To taint the compute instance for recreation on next `apply`:

[source,sh]
----
terraform taint module.main_server.exoscale_compute_instance.cljdoc_01
----

== Creating a Cljdoc Docker Image
The cljdoc docker image runs on the cljdoc host.

[source,sh]
----
bb docker-image
----

This will package the cljdoc application in a Docker container.
A tag will be determined based on number of commits, branch and commit SHA.
Docker images are published to Docker Hub during CI.
See link:/.circleci/config.yml[`.circleci/config.yml`].

[TIP]
====
Run `bb clean` first when testing your image locally.
This will ensure you are not working with stale inputs.
====

== Orchestration (Nomad)

To deploy the cljdoc service to the provisioned infrastructure we use https://www.nomadproject.io[Nomad].
While Nomad provides a convenient CLI interface, it has proven easier to generate Nomad https://www.nomadproject.io/docs/job-specification/index.html[job specs] using Clojure and submit them to the Nomad server via the Nomad REST API.

The relevant code is under link:/ops/exoscale/deploy/[].

Deployment is carried out by CircleCI, see `deploy-to-nomad` job in link:[/.circleci/config.yml]

This will fail unless the https://hub.docker.com/r/cljdoc/cljdoc/[Docker Hub] has a cljdoc image with the provided tag.
The tag names are determined based on Git commit count, branch and HEAD and images are pushed to Docker Hub as part of CI.

[#accessing-nomad]
=== Accessing Nomad

[source,sh]
----
./ops/nomad.sh username@ip
----
Where `username` is your ssh login and `ip` is nomad's IP address.

The script launches an SSH process forwarding port 4646 and 8500.

If you have Nomad installed locally, you can now run `nomad` comands like the following:

[source,sh]
----
nomad status cljdoc
nomad alloc logs -f 683ade58
nomad deployment list
----

Hit ^D to to close the session and forwarded ports.

== Backing Up Data

The SQLite database is automatically backed up daily by cljdoc to Exoscale `cljdoc-backups` bucket.

Our current backup retention strategy is:

* 7 daily
* 4 weekly
* 12 monthly
* 2 yearly

If cljdoc does not find a database on startup, it will automatically restore the most recent one from the `cljdoc-backups` bucket.

== Host

By default the cljdoc web server binds to `localhost`.
This is a safe default for development work.

In production, we run the cljdoc web server from a docker container.
The production docker container launches the cljdoc web server with the `cljdoc.host` JVM system property to override the `localhost` default to `0.0.0.0`.

== SSL Certificates

https://traefik.io[Traefik] generates SSL certificates automatically through Let's Encrypt.

== Checking for Vulnerabilities

Experts will uncover vulnerabilities in some of the technologies we use.
It is inevitable.

We use https://github.com/rm-hull/nvd-clojure[nvd-clojure] to scan cljdoc dependencies for known security issues in our cljdoc docker image.
Run `nvd-check.sh` to launch a scan.
You must specify a NVD database token, get yours here: https://nvd.nist.gov/developers/request-an-api-key

Example usage from cljdoc root:
[source,shell]
----
NVD_API_TOKEN=your-token-here bb nvd-scan
----
Replace `your-token-here` with your actual token.

You'll find reports under `target/nvd/` off the cljdoc project root dir.
The html report is probably the most useful.
Be aware that the scan sometimes reports false positives.
After some careful verification, you can quiet false positives via `nvd-suppresions.xml`.

Other tools such as https://github.com/aquasecurity/trivy[trivy] can identify security holes.
Trivy seems to be good at finding issues in docker images and configuration.
